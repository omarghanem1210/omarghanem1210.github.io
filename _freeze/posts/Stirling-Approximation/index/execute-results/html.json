{
  "hash": "c4d22319caf9b80dec4df39ad4839e31",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A Non-Rigorous Derivation Of The Stirling Approximation\"\ndate: \"2024-05-31\"\ncategories: [probability]\nexecute:\n  echo: false\nimage: factorial.jpg\n---\n\n\n\n\n\nThe Stirling approximation is a way to approximate $x!$ by the seemingly incomprehensible formula:\n\n$$x! \\approx e^{- x} x^{x + \\frac{1}{2}} \\sqrt{2 \\pi}$$\n\nThere is, of course, a completely rigorous way of defining and proving this, rife with integrals, but such a way gives no intuition about *why* such a formula is true. A far better way of proving it goes like this: let $X$ be a binomial random variable which represents the number of successes in $n$ trials with probability $p$ of success. Then we can write $X$ as a sum of $n$ Bernoulli random variables \n\n$$X = B_1 + B_2 + ... + B_n$$\n\nsuch that each $B_i$ is a bernoulli random variable with probability $p$ of success. By the central limit theorem we have that as $n \\to \\infty$, the distribution of $X$ will approach $\\mathcal{N}(np,\\, npq)$, where $q = 1 - p$.\n\nNow here comes the trick, as $n \\to \\infty$, let $p \\to 0$ while keeping $np = \\lambda$, which is a constant. And we all know what the distribution will now tend to right? A poisson distribution with rate $\\lambda$. Also, as we have that $n \\to \\infty$ and $p \\to 0$, the mean of $X$ will approach $\\lambda$ and its variance will also approach $\\lambda$ (If you are wonder why that is the case, take the limit of $npq$ under the given conditions and see what happens). \n\nWe now know two things: the first is that as $n \\to \\infty$ and $p \\to 0$, $p(X) \\approx \\mathcal{N}(\\lambda, \\lambda)$ and also that $p(X) \\approx poisson(\\lambda)$. Therefore, logically, we have that $\\mathcal{N}(\\lambda, \\lambda) \\approx poisson(\\lambda)$, and so\n\n$$\\frac{e^{-\\lambda} \\lambda^x}{x!} \\approx \\frac{1}{\\sqrt{2\\pi\\lambda}} e^{\\frac{-1}{2}(\\frac{x - \\lambda}{ \\lambda})^2}$$\nWe can now substitute $x$ with $\\lambda$ and we will have\n\n$$\\frac{e^{-\\lambda} \\lambda^\\lambda}{\\lambda!} \\approx \\frac{1}{\\sqrt{2\\pi\\lambda}}$$\nAnd the formula that we want follows easily enough\n\n$$\\lambda! \\approx e^{- \\lambda} \\lambda^{\\lambda + \\frac{1}{2}} \\sqrt{2 \\pi}$$\n\n\nThe question now is how large the relative error is? Does it perhaps increases as $\\lambda$ increases? Intuitively, we would guess no, nowhere in our derivation have we assumed anything about the magnitude of $\\lambda$, and the approximation between the normal and poisson distribution holds irrespective of what number $\\lambda$ actually is. But just to feel good, let's check a few values.\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> x </th>\n   <th style=\"text-align:right;\"> x! </th>\n   <th style=\"text-align:right;\"> approximation </th>\n   <th style=\"text-align:right;\"> relative error </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.922137 </td>\n   <td style=\"text-align:right;\"> 0.0778630 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 1.919004 </td>\n   <td style=\"text-align:right;\"> 0.0404978 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 5.836210 </td>\n   <td style=\"text-align:right;\"> 0.0272984 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 24 </td>\n   <td style=\"text-align:right;\"> 23.506175 </td>\n   <td style=\"text-align:right;\"> 0.0205760 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 120 </td>\n   <td style=\"text-align:right;\"> 118.019168 </td>\n   <td style=\"text-align:right;\"> 0.0165069 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 362880 </td>\n   <td style=\"text-align:right;\"> 359536.872842 </td>\n   <td style=\"text-align:right;\"> 0.0092128 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 3628800 </td>\n   <td style=\"text-align:right;\"> 3598695.618741 </td>\n   <td style=\"text-align:right;\"> 0.0082960 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;\"> 479001600 </td>\n   <td style=\"text-align:right;\"> 475687486.472776 </td>\n   <td style=\"text-align:right;\"> 0.0069188 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:right;\"> 355687428096000 </td>\n   <td style=\"text-align:right;\"> 353948328666100.812500 </td>\n   <td style=\"text-align:right;\"> 0.0048894 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 20 </td>\n   <td style=\"text-align:right;\"> 2432902008176640000 </td>\n   <td style=\"text-align:right;\"> 2422786846761135616.000000 </td>\n   <td style=\"text-align:right;\"> 0.0041577 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 33 </td>\n   <td style=\"text-align:right;\"> 8683317618811885938468668682226228200 </td>\n   <td style=\"text-align:right;\"> 8661418381417952528044480808868426880.000000 </td>\n   <td style=\"text-align:right;\"> 0.0025220 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAnd we see that the relative error in fact *decreases* as $x$ increases.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}