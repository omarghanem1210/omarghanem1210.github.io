---
title: "A Non-Rigorous Derivation Of The Stirling Approximation"
date: "2024-06-20"
categories: [probability]
execute:
  echo: false
image: factorial.jpg
---

```{r, include=FALSE}
library(kableExtra)
options(warn=-1)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 


```


The Stirling approximation is a way to approximate $x!$ by the seemingly incomprehensible formula:

$$x! \approx e^{- x} x^{x + \frac{1}{2}} \sqrt{2 \pi}$$

There is, of course, a completely rigorous way of defining and proving this, rife with integrals, but such a way gives no intuition about *why* such a formula is true. A far better way of proving it goes like this: let $X$ be a binomial random variable which represents the number of successes in $n$ trials with probability $p$ of success. Then we can write $X$ as a sum of $n$ Bernoulli random variables 

$$X = B_1 + B_2 + ... + B_n$$

such that each $B_i$ is a bernoulli random variable with probability $p$ of success. By the central limit theorem we have that as $n \to \infty$, the distribution of $X$ will approach $\mathcal{N}(np,\, npq)$, where $q = 1 - p$.

Now here comes the trick, as $n \to \infty$, let $p \to 0$ while keeping $np = \lambda$, which is a constant. And we all know what the distribution will now tend to right? A poisson distribution with rate $\lambda$. Also, as we have that $n \to \infty$ and $p \to 0$, the mean of $X$ will approach $\lambda$ and its variance will also approach $\lambda$ (If you are wonder why that is the case, take the limit of $npq$ under the given conditions and see what happens). 

We now know two things: the first is that as $n \to \infty$ and $p \to 0$, $p(X) \approx \mathcal{N}(\lambda, \lambda)$ and also that $p(X) \approx poisson(\lambda)$. Therefore, logically, we have that $\mathcal{N}(\lambda, \lambda) \approx poisson(\lambda)$, and so

$$\frac{e^{-\lambda} \lambda^x}{x!} \approx \frac{1}{\sqrt{2\pi\lambda}} e^{\frac{-1}{2}(\frac{x - \lambda}{ \lambda})^2}$$
We can now substitute $x$ with $\lambda$ and we will have

$$\frac{e^{-\lambda} \lambda^\lambda}{\lambda!} \approx \frac{1}{\sqrt{2\pi\lambda}}$$
And the formula that we want follows easily enough

$$\lambda! \approx e^{- \lambda} \lambda^{\lambda + \frac{1}{2}} \sqrt{2 \pi}$$


The question now is how large the relative error is? Does it perhaps increases as $\lambda$ increases? Intuitively, we would guess no, nowhere in our derivation have we assumed anything about the magnitude of $\lambda$, and the approximation between the normal and poisson distribution holds irrespective of what number $\lambda$ actually is. But just to feel good, let's check a few values.

```{r}
fact_approx <- function(x){
  return(exp(1)^(-x) * x^(x + 1/2) * sqrt(2 * pi))
}

relative_error <- function(x, x_pred){
  return(abs((x - x_pred) / x))
}


options(scipen = 999)

x <- c(1, 2, 3, 4, 5, 9, 10, 12, 17, 20, 33)

table <- data.frame(x, factorial(x), fact_approx(x), relative_error(factorial(x), fact_approx(x)))

colnames(table) <- c('x', 'x!', 'approximation', 'relative error')

table %>%
  kbl() %>%
  kable_styling()



```

And we see that the relative error in fact *decreases* as $x$ increases.
